<!DOCTYPE html>
<html>
  <head>
    <title>TriDepth</title>
    <meta charset='UTF-8'>
    <link href='style.css' rel='stylesheet' type='text/css'>
    <meta content='width=device-width,initial-scale=1' name='viewport'>
  </head>
  <body>
    <header>
      <div class='header-wrapper'>
        <h1>TriDepth: Triangular Patch-based Deep Depth Prediction</h1>
        <div class='conference'>
          <p><a href='http://www.visualslam.ai/'>ICCV Deep Learning for Visual SLAM Workshop 2019 (oral)</a></p>
        </div>
        <div class='authors clearfix'>
          <p>
            Masaya Kaneko
            <sup>1,2</sup>
          </p>
          <p>
            <a href='https://kensakurada.github.io'>
              Ken Sakurada
            </a>
            <sup>2</sup>
          </p>
          <p>
            <a href='https://www.hal.t.u-tokyo.ac.jp/~aizawa/'>
              Kiyoharu Aizawa
            </a>
            <sup>1</sup>
          </p>
        </div>
        <div class='belongs'>
          <p>
            The University of Tokyo
            <sup>1</sup>
          </p>
          <p>
            National Institute of Advanced Industrial Science and Technology (AIST)
            <sup>2</sup>
          </p>
        </div>
        <div class='header-demo'>
          <ul class='clearfix'>
            <li>
              <div>
                <p>Input image</p>
              </div>
              <img src='MeshDepth_files/000093_rgb.png'>
            </li>
            <li>
              <div>
                <p>Depthmap</p>
              </div>
              <img src='MeshDepth_files/000093_ours.png'>
            </li>
            <li>
              <div>
                <p>Triangular-patch-cloud</p>
              </div>
              <img src='MeshDepth_files/000093_patchcloud.gif'>
            </li>
            <li>
              <div>
                <p>3D mesh</p>
              </div>
              <img src='MeshDepth_files/000093_mesh.gif'>
            </li>
          </ul>
        </div>
        <div class='abstract'>
          <h2>Abstract</h2>
          <p>We propose a novel and efficient representation for single-view depth estimation using Convolutional Neural Networks (CNNs). Point-cloud is generally used for CNN-based 3D scene reconstruction; however it has some drawbacks: (1) it is redundant as a representation for planar surfaces, and (2) no spatial relationships between points are available (e.g, texture and surface). As a more efficient representation, we introduce a triangular-patch-cloud, which represents the surface of the 3D structure using a set of triangular patches, and propose a CNN framework for its 3D structure estimation. In our framework, we create it by separating all the faces in a 2D mesh, which are determined adaptively from the input image, and estimate depths and normals of all the faces. Using a common RGBD-dataset, we show that our representation has a better or comparable performance than the existing point-cloud-based methods, although it has much less parameters.</p>
          <div class='files clearfix'>
            <p>
              [
              <a href='http://openaccess.thecvf.com/content_ICCVW_2019/papers/DL4VSLAM/Kaneko_TriDepth_Triangular_Patch-Based_Deep_Depth_Prediction_ICCVW_2019_paper.pdf'>
                Paper
              </a>
              ]
            </p>
            <p>
              [
              <a href='https://github.com/syinari0123/tridepth'>
                Code
              </a>
              ]
            </p>
          </div>
        </div>
      </div>
    </header>
    <div class='contents'>
      <div class='contents-wrapper'>
        <section>
          <h2>Triangular-patch-cloud</h2>
          <p>
            We introduce a novel representation, triangle-patch-cloud, composed of triangular patches.
            It is created by separating all the faces in a 2D mesh, adaptively determined by input image.
            All of those patche's 3D position (depth + normal) are predicted by CNN.
            This representation is more efficient than point-cloud representation, while its depth map has better or comparable performance than those of existing CNN-based poin-cloud methods.
            <img src='MeshDepth_files/overview.jpg', class="overview-img">
          </p>
        </section>
        <section>
          <h2>Depth map Prediction</h2>
          <p>
            Quantitative (top) and qualitative (bottom) results showing our depth map rendered from predicted triangular-patch-cloud. 
            We evaluate our method using NYU Depth v2 dataset [Silberman <i>et al.</i>, ECCV'12], and evaluate its depth map using general error metrics.
            Our results achieved better or comparable performance to that of the existing deep depth prediction methods ([Eigen <i>et al.</i>, ICCV'15], [Laina <i>et al.</i>, 3DV'16], [Fu <i>et al.</i>, CVPR'18]), 
            despite having much less parameters.
          </p>
          <div class='table-cover'>
            <table>
              <thead>
                <tr class='border-under'>
                  <th>Method</th>
                  <th>RMSE</th>
                  <th>IRMSE</th>
                  <th>AbsRel</th>
                  <th>delta1</th>
                  <th>delta2</th>
                  <th>delta3</th>
                  <th>#Param.</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>[Eigen <i>et al.</i>, ICCV'15]</td>
                  <td>.5781</td>
                  <td>.1028</td>
                  <td>.1635</td>
                  <td>.7533</td>
                  <td>.9444</td>
                  <td>.9866</td>
                  <td>307K</td>
                </tr>
                  <td>[Laina <i>et al.</i>, 3DV'16]</td>
                  <td>.5112</td>
                  <td>.0933</td>
                  <td>.1368</td>
                  <td>.8149</td>
                  <td>.9538</td>
                  <td>.9881</td>
                  <td>307K</td>
                <tr class='border-under'>
                  <td>[Fu <i>et al.</i>, CVPR'18]</td>
                  <td>.5459</td>
                  <td>.1007</td>
                  <td><b>.1287</b></td>
                  <td><b>.8258</b></td>
                  <td>.9431</td>
                  <td>.9776</td>
                  <td>307K</td>
                </tr>
                <tr>
                  <td>Ours</td>
                  <td><b>.5102</b></td>
                  <td><b>.0905</b></td>
                  <td>.1411</td>
                  <td>.8178</td>
                  <td><b>.9577</b></td>
                  <td><b>.9893</b></td>
                  <td><b>30K</b></td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>Since our depth map is created by the 2D mesh extracted based on the canny edge, its object boundaries are clearer than those of the pixel-wise deep depth prediction methods.</p>
          <table class='compare-images'>
            <thead>
              <tr>
                <th>Input image</th>
                <th>GT</th>
                <th>Ours</th>
                <th>Laina <i>et al.</i></th>
                <th>Fu <i>et al.</i></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><img src='MeshDepth_files/depthmap/000158.png'></td>
                <td><img src='MeshDepth_files/depthmap/000158_gt.png'></td>
                <td><img src='MeshDepth_files/depthmap/000158_ours.png'></td>
                <td><img src='MeshDepth_files/depthmap/000158_laina_raw.png'></td>
                <td><img src='MeshDepth_files/depthmap/000158_dorn_raw.png'></td>
              </tr>
              <tr>
                <td><img src='MeshDepth_files/depthmap/000573.png'></td>
                <td><img src='MeshDepth_files/depthmap/000573_gt.png'></td>
                <td><img src='MeshDepth_files/depthmap/000573_ours.png'></td>
                <td><img src='MeshDepth_files/depthmap/000573_laina_raw.png'></td>
                <td><img src='MeshDepth_files/depthmap/000573_dorn_raw.png'></td>
              </tr>
            </tbody>
          </table>
        </section>
        <section>
          <h2>3D Structure Visualization</h2>
          <p>
            These are our predictions of 3D triangular-patch-cloud (top) and its 3D mesh (bottom).
            3D mesh can be easily obtained from our triangular-patch-cloud by connecting the adjacent neighbor patches.
            This process can not only reduce the parameter size (the number of vertex) of 3D structure, but also make visualization result clean.
          </p>
          <table class='compare-vis'>
            <tbody>
              <tr>
                <td><img src='MeshDepth_files/patch_cloud/000221_tpc.gif'></td>
                <td><img src='MeshDepth_files/patch_cloud/000255_tpc.gif'></td>
                <td><img src='MeshDepth_files/patch_cloud/000513_tpc.gif'></td>
              </tr>
              <tr>
                <td><img src='MeshDepth_files/patch_cloud/000221.gif'></td>
                <td><img src='MeshDepth_files/patch_cloud/000255.gif'></td>
                <td><img src='MeshDepth_files/patch_cloud/000513.gif'></td>
              </tr>
            </tbody>
          </table>
          <p>
            Our method works well in other indoor scenes. These are some results of our 3D mesh prediction.
          </p>
          <div class='demo-images'>
            <img src='MeshDepth_files/3d_mesh/000058.gif'>
            <img src='MeshDepth_files/3d_mesh/000087.gif'>
            <img src='MeshDepth_files/3d_mesh/000158.gif'>
            <img src='MeshDepth_files/3d_mesh/000179.gif'>
            <img src='MeshDepth_files/3d_mesh/000223.gif'>
            <img src='MeshDepth_files/3d_mesh/000333.gif'>
            <img src='MeshDepth_files/3d_mesh/000442.gif'>
            <img src='MeshDepth_files/3d_mesh/000466.gif'>
            <img src='MeshDepth_files/3d_mesh/000480.gif'>
            <img src='MeshDepth_files/3d_mesh/000504.gif'>
            <img src='MeshDepth_files/3d_mesh/000524.gif'>
            <img src='MeshDepth_files/3d_mesh/000534.gif'>
            <img src='MeshDepth_files/3d_mesh/000561.gif'>
            <img src='MeshDepth_files/3d_mesh/000580.gif'>
            <img src='MeshDepth_files/3d_mesh/000628.gif'>
          </div>
        </section>
        <section>
          <h2>Video</h2>
          <div class='video'>
            <iframe allow='accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture' allowfullscrean frameborder='0' height='315' src='https://www.youtube.com/embed/w8qmmqatQgY' width='560'></iframe>
          </div>
        </section>
        <section>
          <h2>Citation</h2>
<pre>
@misc{kaneko19tridepth,
  Author = {Masaya Kaneko and Ken Sakurada and Kiyoharu Aizawa},
  Title = {TriDepth: Triangular Patch-based Deep Depth Prediction},
  Booktitle = {ICCV Deep Learning for Visual SLAM Workshop},
  Year = {2019},
}
</pre>
        </section>
        <section>
          <h2>Acknowledgements</h2>
          <p>
            We thank 
            <a href='https://seishin55.com/'>
              Shizuma Kubo
            </a>
            for modifying the code of this webpage, whose template design was borrowed from
            <a href='https://keypointnet.github.io/'>
              keypointnet webpage
            </a>
            . 
          </p>
        </section>
      </div>
    </div>
  </body>
</html>
