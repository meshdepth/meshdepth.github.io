<!DOCTYPE html>
<html>
  <head>
    <title>TriDepth</title>
    <meta charset='UTF-8'>
    <link href='style.css' rel='stylesheet' type='text/css'>
    <meta content='width=device-width,initial-scale=1' name='viewport'>
  </head>
  <body>
    <header>
      <div class='header-wrapper'>
        <h1>TriDepth: Triangular Patch-based Deep Depth Prediction</h1>
        <div class='conference'>
          <p><a href='http://www.visualslam.ai/'>ICCV Deep Learning for Visual SLAM Workshop 2019 (oral)</a></p>
        </div>
        <div class='authors clearfix'>
          <p>
            Masaya Kaneko
            <sup>1,2</sup>
          </p>
          <p>
            <a href='https://kensakurada.github.io'>
              Ken Sakurada
            </a>
            <sup>2</sup>
          </p>
          <p>
            <a href='https://www.hal.t.u-tokyo.ac.jp/~aizawa/'>
              Kiyoharu Aizawa
            </a>
            <sup>1</sup>
          </p>
        </div>
        <div class='belongs'>
          <p>
            The University of Tokyo
            <sup>1</sup>
          </p>
          <p>
            National Institute of Advanced Industrial Science and Technology (AIST)
            <sup>2</sup>
          </p>
        </div>
        <div class='header-demo'>
          <ul class='clearfix'>
            <li>
              <p>Input image</p>
              <img src='MeshDepth_files/000093_rgb.png'>
            </li>
            <li>
              <p>Depthmap</p>
              <img src='MeshDepth_files/000093_ours.png'>
            </li>
            <li>
              <p>Triangular-patch-cloud</p>
              <img src='MeshDepth_files/000093_mesh.gif'>
            </li>
            <li>
              <p>3D mesh</p>
              <img src='MeshDepth_files/000093_mesh.gif'>
            </li>
          </ul>
        </div>
        <div class='abstract'>
          <h2>Abstract</h2>
          <p>We propose a novel and efficient representation for single-view depth estimation using Convolutional Neural Networks (CNNs). Point-cloud is generally used for CNN-based 3D scene reconstruction; however it has some drawbacks: (1) it is redundant as a representation for planar surfaces, and (2) no spatial relationships between points are available (e.g, texture and surface). As a more efficient representation, we introduce a triangular-patch-cloud, which represents the surface of the 3D structure using a set of triangular patches, and propose a CNN framework for its 3D structure estimation. In our framework, we create it by separating all the faces in a 2D mesh, which are determined adaptively from the input image, and estimate depths and normals of all the faces. Using a common RGBD-dataset, we show that our representation has a better or comparable performance than the existing point-cloud-based methods, although it has much less parameters.</p>
          <div class='files clearfix'>
            <p>
              [
              <a href='http://openaccess.thecvf.com/content_ICCVW_2019/papers/DL4VSLAM/Kaneko_TriDepth_Triangular_Patch-Based_Deep_Depth_Prediction_ICCVW_2019_paper.pdf'>
                Paper
              </a>
              ]
            </p>
            <p>
              [
              <a href='https://github.com/syinari0123/tridepth'>
                Code
              </a>
              ]
            </p>
          </div>
        </div>
      </div>
    </header>
    <div class='contents'>
      <div class='contents-wrapper'>
        <section>
          <h2>Depthmap Prediction</h2>
          <p>
            Quantitative (top) and qualitative (bottom) results showing our depthmap rendered from predicted 3D mesh. Our results achieved the best or comparabe performance to that of the state-of-the-art pixel-wise dense methods (Eigen
            <i>et al.</i>
            and Laina
            <i>et al.</i>
            ), despite the mesh representation using much less parameters.
          </p>
          <div class='table-cover'>
            <table>
              <thead>
                <tr class='border-under'>
                  <th>Method</th>
                  <th>rel</th>
                  <th>rms</th>
                  <th>log10</th>
                  <th>delta1</th>
                  <th>delta2</th>
                  <th>delta3</th>
                  <th>#param.</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    Eigen
                    <i>et al.</i>
                  </td>
                  <td>.158</td>
                  <td>.641</td>
                  <td>-</td>
                  <td>.769</td>
                  <td>.950</td>
                  <td>
                    <b>.988</b>
                  </td>
                  <td>921K</td>
                </tr>
                <tr class='border-under'>
                  <td>
                    Laina
                    <i>et al.</i>
                  </td>
                  <td>
                    <b>.127</b>
                  </td>
                  <td>.573</td>
                  <td>
                    <b>.055</b>
                  </td>
                  <td>
                    <b>.811</b>
                  </td>
                  <td>.953</td>
                  <td>
                    <b>.988</b>
                  </td>
                  <td>921K</td>
                </tr>
                <tr>
                  <td>
                    Ours
                  </td>
                  <td>.146</td>
                  <td>
                    <b>.530</b>
                  </td>
                  <td>.062</td>
                  <td>.803</td>
                  <td>
                    <b>.954</b>
                  </td>
                  <td>
                    <b>.988</b>
                  </td>
                  <td>
                    <b>32K</b>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>Since our depthmap is created by the 2D mesh extracted based on the canny edge, the object boundaries of our depthmap is clearer than that of the pixel-wise method.</p>
          <table class='compare-images'>
            <thead>
              <tr>
                <th>Input image</th>
                <th>GT</th>
                <th>Ours</th>
                <th>
                  Eigen
                  <i>et al.</i>
                </th>
                <th>
                  Laina
                  <i>et al.</i>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <img src='MeshDepth_files/depthmap/000150_rgb.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000150_gt.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000150_ours.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000150_eigen_ori.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000150_laina_ori.png'>
                </td>
              </tr>
              <tr>
                <td>
                  <img src='MeshDepth_files/depthmap/000158_rgb.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000158_gt.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000158_ours.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000158_eigen_ori.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000158_laina_ori.png'>
                </td>
              </tr>
              <tr>
                <td>
                  <img src='MeshDepth_files/depthmap/000573_rgb.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000573_gt.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000573_ours.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000573_eigen_ori.png'>
                </td>
                <td>
                  <img src='MeshDepth_files/depthmap/000573_laina_ori.png'>
                </td>
              </tr>
            </tbody>
          </table>
        </section>
        <section>
          <h2>3D Mesh Prediction</h2>
          <p>
            These are 3D mesh predictions from a single-view image.
            <br>
            Our CNN framework can be trained end-to-end from general RGBD datasets without GT mesh data.
          </p>
          <div class='demo-images'>
            <img src='MeshDepth_files/3d_mesh/000058.gif'>
            <img src='MeshDepth_files/3d_mesh/000087.gif'>
            <img src='MeshDepth_files/3d_mesh/000158.gif'>
            <img src='MeshDepth_files/3d_mesh/000179.gif'>
            <img src='MeshDepth_files/3d_mesh/000223.gif'>
            <img src='MeshDepth_files/3d_mesh/000333.gif'>
            <img src='MeshDepth_files/3d_mesh/000442.gif'>
            <img src='MeshDepth_files/3d_mesh/000466.gif'>
            <img src='MeshDepth_files/3d_mesh/000480.gif'>
            <img src='MeshDepth_files/3d_mesh/000504.gif'>
            <img src='MeshDepth_files/3d_mesh/000524.gif'>
            <img src='MeshDepth_files/3d_mesh/000534.gif'>
            <img src='MeshDepth_files/3d_mesh/000561.gif'>
            <img src='MeshDepth_files/3d_mesh/000580.gif'>
            <img src='MeshDepth_files/3d_mesh/000628.gif'>
          </div>
        </section>
        <section>
          <h2>Video</h2>
          <div class='video'>
            <iframe allow='accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture' allowfullscrean frameborder='0' height='315' src='https://www.youtube.com/embed/ZPG6UFpYcI4' width='560'></iframe>
          </div>
        </section>
        <section>
          <h2>Citation</h2>
<pre>
@misc{kaneko19tridepth,
  Author = {Masaya Kaneko and Ken Sakurada and Kiyoharu Aizawa},
  Title = {TriDepth: Triangular Patch-based Deep Depth Prediction},
  Booktitle = {ICCV Deep Learning for Visual SLAM Workshop},
  Year = {2019},
}
</pre>
        </section>
        <section>
          <h2>Acknowledgements</h2>
          <p>
            We thank 
            <a href='https://seishin55.com/'>
              Shizuma Kubo
            </a>
            for modifying the code of this webpage, whose template design was borrowed from
            <a href='https://keypointnet.github.io/'>
              keypointnet webpage
            </a>
            . 
          </p>
        </section>
      </div>
    </div>
  </body>
</html>
